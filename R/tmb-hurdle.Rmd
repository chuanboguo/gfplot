# Evaluating approaches to standardizing groundfish commercial CPUE

```{r, eval=TRUE, echo=FALSE}
knitr::opts_knit$set(root.dir = '..')
ggplot2::theme_set(ggplot2::theme_light())
```

Let's start by simulating some simple data to fit a delta-lognormal model to and verify that we get comparable confidence intervals to the bootstrap approach.

```{r, message=FALSE, warning=FALSE}
library(ggplot2)
library(dplyr)
set.seed(1)
x <- runif(500, -2, 2)
lp <- -0.9 + x * 0.5
y_bin <- rbinom(length(x), size = 1, prob = plogis(lp))
y <- y_bin * exp(rnorm(length(x), lp, 0.2))
non_zero <- ifelse(y > 0, 1, 0)
d <- data.frame(x, y, non_zero)
d <- mutate(d, y_to_plot = ifelse(non_zero == 0, y + jitter(y, amount = 0.02), y)) 
ggplot(d, aes(x, y_to_plot, colour = as.factor(non_zero))) + 
  geom_point(alpha = 0.6)
```

Now we can fit models to the simulated data and combine them by multiplying the probability of observing something by the expectation given that something is observed. Note that I am not dealing with bias correction when exponentiating the projections from the linear model fit to log data. This should be considered.

```{r}
m1 <- glm(non_zero ~ x, data = d, family = binomial(link = logit))
m2 <- lm(log(y) ~ x, data = subset(d, non_zero == 1))
pos_dat <- subset(d, non_zero == 1)
bin_coef <- plogis(predict(m1, newdata = data.frame(x = 0)))
gamma_coef <- exp(predict(m2, newdata = data.frame(x = 0)))
bin_coef * gamma_coef
```

## A delta-lognormal model fit in TMB

Now let's load our TMB version and compare the results.

This is the model:

```{r, comment=NA,echo=FALSE}
writeLines(readLines("deltalognormal.cpp"))
```

```{r, message=FALSE, warning=FALSE, results="hide"}
library(TMB)
compile("deltalognormal.cpp")
dyn.load(dynlib("deltalognormal"))

mm1 <- as.matrix(model.matrix(non_zero ~ x, data = d))
mm2 <- as.matrix(model.matrix(y ~ x, data = pos_dat))
mm_pred <- matrix(nrow = 1, ncol = 2, data = c(1, 0))
obj <- MakeADFun(
  data = list(
    X1_ij = mm1, y1_i = d$non_zero,
    X2_ij = mm2, y2_i = log(pos_dat$y),
    X1_pred_ij = mm_pred, X2_pred_ij = mm_pred),
  parameters = list(b1_j = rep(0, ncol(mm1)), b2_j = rep(0, ncol(mm2)),
    log_sigma = log(0.2)),
  DLL = "deltalognormal")
obj$fn(obj$par)
obj$gr(obj$par)
opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr)
r <- sdreport(obj)
```

```{r}
summary(r)
exp(summary(r)["log_prediction","Estimate"])
exp(summary(r)["log_prediction","Estimate"] +
    1.96 * summary(r)["log_prediction","Std. Error"])
exp(summary(r)["log_prediction","Estimate"] -
    1.96 * summary(r)["log_prediction","Std. Error"])
```

We get exactly the same point estimate as with the R functions.

We can compare the confidence intervals derived from the delta method standard errors from TMB (on the log scale) to a bootstrap approach:

```{r, cache=TRUE}
hurdle_fn <- function(data, i) {
  dat_boot <- data[i, ]
  m1 <- glm(non_zero ~ x, data = dat_boot,
    family = binomial(link = logit))
  m2 <- glm(y ~ x, data = subset(dat_boot, non_zero == 1),
    family = Gamma(link = log))
  bin_coef <- plogis(predict(m1, newdata = data.frame(x = 0)))
  gamma_coef <- exp(predict(m2, newdata = data.frame(x = 0)))
  bin_coef * gamma_coef
}

library(boot)
b <- boot(d, hurdle_fn, R = 1000)
b.ci <- boot.ci(b, type = "bca")
print(b.ci)
```

The confidence intervals are almost exactly the same. However, the bootstrap approach takes a very long time to fit to a full data set of real groundfish CPUE data. It's doable for one stock if you are very patient but likely prohibitive to run for all or many stocks. The TMB approach is much faster.

## A delta-lognormal TMB GLM fit to real CPUE data

Now let's try fitting our model to some real CPUE data. As an example, I will work with bottom trawl CPUE for walleye pollock in 5CDE so we have a point of comparison from the recent assessment. First I will use a GLM and LM (linear model) as above to relatively closely match the approach that was used in the assessment.

```{r}
grep_pattern <- "5[CDE]+"
if (!file.exists("d_retained.rda")) {
  source("R/cpue-index.R")
  save(d_retained, file = "d_retained.rda") 
} else {
  load("d_retained.rda")
}
nrow(d_retained)
mean(d_retained$pos_catch)
```

Let's fit our models in TMB. First we will start with a simple model that just has year effects. 

```{r, message=FALSE, warning=FALSE, results="hide"}
compile("deltalognormal.cpp")
dyn.load(dynlib("deltalognormal"))

f1 <- pos_catch ~ year_factor
f2 <- log(spp_catch/hours_fished) ~ year_factor

if (!file.exists("cpue-glm-eg-year.rda")) {
  source("R/delta-cpue-tmb-glm.R")
  save(r, mm1, mm2, obj, file = "cpue-glm-eg-year.rda") 
} else {
  load("cpue-glm-eg-year.rda")
}
```

```{r}
gg_tmb_cpue_ribbon <- function(report) {
  report_sum <- summary(report)
  ii <- grep("log_prediction", row.names(report_sum))
  row.names(report_sum) <- NULL
  df <- as.data.frame(report_sum[ii, ])
  df$year <- 1996:max(d_retained$year)
  
  df$Estimate <- df$Estimate - mean(df$Estimate)
  g <- ggplot(df, aes(year, exp(Estimate),
    ymin = exp(Estimate - 1.96 * `Std. Error`),
    ymax = exp(Estimate + 1.96 * `Std. Error`))) +
    geom_ribbon(alpha = 0.5) +
    geom_line()
  print(g)
  invisible(df)
}
df1 <- gg_tmb_cpue_ribbon(r)
```

Now we can fit a full model that matches the model used in the last stock assessment:

```{r, message=FALSE, warning=FALSE, results="hide"}
f <- function(x) as.factor(as.character(x))
f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
  f(dfo_locality) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(vessel_name) + f(depth_band) + f(latitude_band) +
  f(dfo_locality)

if (!file.exists("cpue-glm-eg.rda")) {
  source("R/delta-cpue-tmb-glm.R")
  save(r, mm1, mm2, obj, file = "cpue-glm-eg.rda") 
} else {
  load("cpue-glm-eg.rda")
}
```

```{r}
opt$convergence
max(r$gradient.fixed)
r2 <- r
df2 <- gg_tmb_cpue_ribbon(r2)
sm <- summary(r2)
```

Let's combine the predictions from the earlier simple model with this model to plot them. The model with just year effects is in blue:

```{r}
ggplot(df2, aes(year, exp(Estimate),
  ymin = exp(Estimate - 1.96 * `Std. Error`),
  ymax = exp(Estimate + 1.96 * `Std. Error`))) +
  geom_ribbon(alpha = 0.3) +
  geom_line() +
  geom_line(data = df1, col = "blue") +
  geom_ribbon(data = df1, fill = "blue", alpha = 0.3)
```

I also wrote a helper function to plot the coefficients from the models. The panels that start with "bin" are the binary models and the panel to start with "pos" are the positive component models. I'd have to do a bit more work to make sure the various binned factor predictors are always in order in the plot.

<!-- TODO -->

```{r, fig.height=12, fig.width=12}
source("R/gg_cpue_coefs.R")
gg_cpue_coefs(sm, mm1, mm2, re = FALSE)
```

## Evaluating the sensitivity to individual predictors

Let's try jackknifing out each of the predictor variables one at a time to see their influence on the final index trend. 

```{r jk, cache=TRUE}
f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
  f(dfo_locality) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(vessel_name) + f(depth_band) + f(latitude_band) +
  f(dfo_locality)

make_pred_mm <- function(x) {
  mm_pred <- x[1:length(unique(d_retained$year)), ]
  for (i in 1:ncol(mm_pred)) {
    for (j in 1:nrow(mm_pred)) {
      mm_pred[j, i] <- 0
    }}
  mm_pred[,1] <- 1
  for (i in 1:ncol(mm_pred)) {
    for (j in 1:nrow(mm_pred)) {
      if (i == j)
        mm_pred[j, i] <- 1
    }}
  mm_pred
}

jackknife_cpue <- function(f_bin, f_pos, terms) {
  
  mm1 <- model.matrix(f_bin, data = d_retained)
  mm2 <- model.matrix(f_pos, data = subset(d_retained, pos_catch == 1))
  mm1 <- make_pred_mm(mm1)
  mm2 <- make_pred_mm(mm2)
  
  m_bin <- speedglm::speedglm(f_bin, data = d_retained,
    family = binomial(link = "logit"))
  m_pos <- lm(f_pos, data = subset(d_retained, pos_catch == 1))
  
  p1 <- plogis(mm1 %*% coef(m_bin))
  p2 <- exp(mm2 %*% coef(m_pos))
  full <- data.frame(year = 1996:2015, term = "all", pred = p1 * p2)
  
  fitm <- function(drop_term) {
    f1 <- update.formula(formula(m_bin), as.formula(paste0(". ~ . -", drop_term)))
    f2 <- update.formula(formula(m_pos), as.formula(paste0(". ~ . -", drop_term)))
    mm1 <- model.matrix(f1, data = d_retained)
    mm2 <- model.matrix(f2, data = subset(d_retained, pos_catch == 1))
    mm1 <- make_pred_mm(mm1)
    mm2 <- make_pred_mm(mm2)
    m_bin <- speedglm::speedglm(f1, data = d_retained,
      family = binomial(link = "logit"))
    m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
    p1 <- plogis(mm1 %*% coef(m_bin))
    p2 <- exp(mm2 %*% coef(m_pos))
    o <- data.frame(year = 1996:2015, term = drop_term, pred = p1 * p2)
    o
  }
  
  out <- plyr::ldply(terms, fitm)
  suppressWarnings(out <- bind_rows(full, out))
  out <- group_by(out, term) %>% mutate(pred = pred / exp(mean(log(pred)))) %>% 
      ungroup()
  out
}

jk <- jackknife_cpue(f1, f2, terms = c("f(depth_band)", "f(vessel_name)", 
  "f(latitude_band)", "f(dfo_locality)", "f(month_factor)"))
```

```{r, dependson='jk'}
ggplot(jk, aes(year, pred, colour = term)) +
  geom_line(data = df1, aes(y = exp(Estimate - mean(Estimate))), colour = "black", lty = 2) +
  geom_line(aes(size = term)) +
  scale_color_manual(values = c("Black", RColorBrewer::brewer.pal(8, "Set2"))) +
  scale_size_manual(values = c(3, rep(0.75, 99)))
```

In the above plot, the dashed line is the unstandardized CPUE trend. The thick black line is the model with all of the predictors. Each of the colored lines represents a model with that individual term removed from the model.

Lines that deviate the furthest from the overall thick black line have the biggest influence on the model fit. From this plot we can see that depth has the biggest influence on standardizing the index series. Leaving out depth results in a index that isn't that far off from the unstandardized CPUE index series. DFO locality has a relatively big effect on the index in recent years. 

```{r eval=FALSE, include=FALSE}
# # coefs AIC:
# m_bin <- speedglm::speedglm(f1, data = d_retained, family = binomial(link = "logit"))
# m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
#
# m_bin2 <- update(m_bin, . ~ . + poly(best_depth, 4) - f(depth_band))
# bbmle::AICtab(m_bin, m_bin2)
#
# m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
# m_pos2 <- update(m_pos, . ~ . + poly(best_depth, 8) - f(depth_band))
# bbmle::AICtab(m_pos, m_pos2)

# Let's compare that to a bootstrap approach:
# 
# f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
#   f(dfo_locality) + f(depth_band) + f(latitude_band)
# f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
#   f(vessel_name) + f(depth_band) + f(latitude_band) +
#   f(dfo_locality)
# hurdle_fn <- function(data, i) {
#  dat_boot <- data[i, ]
#  # m_bin <- speedglm::speedglm(f1, data = d_retained,
#    # family = binomial(link = "logit"))
#  m_pos <- lm(f2, data = subset(dat_boot, pos_catch == 1))
#  pos_coef <- coef(m_pos)[1:20]
#  pos_coef
# }
# library(boot)
# b <- boot(d_retained, hurdle_fn, R = 100)
# b.ci <- boot.ci(b, type = "bca")
# b
# print(b.ci)

# Before calling mclapply do openmp(1) to avoid forking a multithreaded process
# TMB::openmp(1)
# parallel::mclapply(1:10,function(x)do.call("optim",obj))
# parallel_accumulator<Type> res(this);
# parallel_accumulator<Type> nll(this);
```

## A random effects model

Now let's fit a similar model with random intercepts for vessel and DFO locality instead of treating them as factor level predictors.

This is the model:

```{r, comment=NA,echo=FALSE}
writeLines(readLines("deltalognormal_2re.cpp"))
```

```{r, message=FALSE, warning=FALSE, results="hide"}
compile("deltalognormal_2re.cpp")
dyn.load(dynlib("deltalognormal_2re"))

f1 <- pos_catch ~ year_factor + f(month_factor) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(depth_band) + f(latitude_band)

if (!file.exists("cpue-glmm-eg.rda")) {
  source("R/delta-cpue-tmb-glmm.R")
  save(r_re, mm1, mm2, obj, file = "cpue-glmm-eg.rda") 
} else {
  load("cpue-glmm-eg.rda")
}
```

That takes considerably longer to run (several minutes). Let's make a plot combining the various trends we have calculated:

```{r}
df_re <- gg_tmb_cpue_ribbon(r_re)
sm_re <- summary(r_re)
ggplot(df2, aes(year, exp(Estimate),
  ymin = exp(Estimate - 1.96 * `Std. Error`),
  ymax = exp(Estimate + 1.96 * `Std. Error`))) +
  geom_ribbon(data = df_re, fill =  "red", alpha = 0.3) + 
  geom_ribbon(data = df1, fill = "blue", alpha = 0.3) +
  geom_ribbon(alpha = 0.3) +
  geom_line(data = df_re, col = "red") +
  geom_line(data = df1, col = "blue") +
  geom_line()
```

In the above plot, blue is the unstandardized series, black is the GLM/LM standardized series, and red is the GLMM/LMM standardized series (i.e. the red line and shading is for the model including random intercepts for vessels and DFO locality).

So the random effects model produces almost exactly the same point estimate of the index trend but introduces a bit of (likely realistic) uncertainty around the trend estimate. Importantly, the random effect model estimates its trend for an average locality and average vessel whereas the GLM version has to make its estimate at a particular locality and vessel. However, once they are all scaled to have the same geometric mean the results are quite similar. 

Again, let's plot out the coefficients:

```{r, fig.height=12, fig.width=12}
gg_cpue_coefs(sm_re, mm1, mm2, re = TRUE)
```

# Comparing methods of combining positive and binary component models

I've been trying to work out the formula used to combine the positive and binary models in a number of recent groundfish stock assessments. The following is a tiny simulation example:

```{r}
cyy <- function(p0, byy, lyy) {lyy / (1 - p0 * (1 - 1/(byy)))}
cyy_lower <- function(p0, byy) {(1 - p0 * (1 - 1/(byy)))}

set.seed(1)
p <- rnorm(20, mean = 0)
b <- rnorm(20, mean = 1)
plot(scale(cyy(plogis(p[1]), plogis(b), exp(p))), type = "l", lty = 2, col = "red")
lines(scale(plogis(b) * exp(p)), type = "l", lty = 1)
```

In the above plot, the red line is the version combined according to the formula and the black line is the version combined by multiplying the probability by the positive mean expectation. The results are similar (after scaling) but not exactly the same. 

We can also just plot out of the lower half of the formula that scales and inverts the probability component to see what's going on.

```{r}
plot(1/(cyy_lower(plogis(p[1]), plogis(b))), type = "l", lty = 2, col = "red",
  ylim = c(0, 1))
lines(plogis(b))
```

In the above plot, the red line is the version from the formula and the black line is 1 minus the raw probability. I still can't wrap my head around why we'd want to use the formula or method used in a number of recent assessments versus just multiplying the probability by the positive estimate.

