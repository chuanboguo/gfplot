# Evaluating approaches to standardizing groundfish commercial CPUE

The purpose of this document is to:

1. make sure I can reproduce the CPUE index standardization results from recent assessments so I can create them for many stocks and regions for the data synopsis report
2. test a delta-lognormal model written in TMB to see if it's considerably faster and obtains the same results
3. try a version of the delta-lognormal model that includes random intercepts to see if this is technically feasible in terms of computation time and to see whether it generates a similar standardized index

To jump to the punchline:

1. yes
2. yes
3. similar result, 10 times slower

In the section below using real data I focus on walleye pollock bottom trawl CPUE in areas 5CDE. In terms of vessel selection criteria right now I am using the same criteria used in the recent walleye pollock assessment:

- vessels must catch pollock in at least 4 years
- each of those years must include at least 4 trips with pollock
- each included vessel must make at least 100 trips that include some pollock
- include all trips for retained vessels

```{r, eval=TRUE, echo=FALSE}
knitr::opts_knit$set(root.dir = '..')
ggplot2::theme_set(ggplot2::theme_light())
```

Let's start by simulating some simple data to fit a delta-lognormal model to and verify that we get comparable confidence intervals to the bootstrap approach.

```{r, message=FALSE, warning=FALSE}
library(tidyverse)
set.seed(1)
x <- runif(10000, -2, 2)
lp <- -0.9 + x * 0.5
y_bin <- rbinom(length(x), size = 1, prob = plogis(lp))
y <- y_bin * exp(rnorm(length(x), lp, 0.2))
non_zero <- ifelse(y > 0, 1, 0)
d <- data.frame(x, y, non_zero)
d <- mutate(d, y_to_plot = ifelse(non_zero == 0, y + jitter(y, amount = 0.03), y)) 
ggplot(d, aes(x, y_to_plot, colour = as.factor(non_zero))) + 
  geom_point(alpha = 0.4)
```

Now we can fit models to the simulated data and combine them by multiplying the probability of observing something by the expectation given that something is observed. Note that I am not dealing with bias correction when exponentiating the predicitons from the linear model fit to log data. This should be considered later.

```{r}
m1 <- glm(non_zero ~ x, data = d, family = binomial(link = logit))
m2 <- lm(log(y) ~ x, data = subset(d, non_zero == 1))
pos_dat <- subset(d, non_zero == 1)
bin_coef <- plogis(predict(m1, newdata = data.frame(x = 0)))
gamma_coef <- exp(predict(m2, newdata = data.frame(x = 0)))
bin_coef * gamma_coef
```

## A delta-lognormal model fit in TMB

Now let's load our TMB version and compare the results.

This is the model:

```{r, comment=NA,echo=FALSE}
writeLines(readLines("inst/tmb/deltalognormal.cpp"))
```

```{r, message=FALSE, warning=FALSE, results="hide"}
library(TMB)
compile("inst/tmb/deltalognormal.cpp")
dyn.load(dynlib("inst/tmb/deltalognormal"))

mm1 <- as.matrix(model.matrix(non_zero ~ x, data = d))
mm2 <- as.matrix(model.matrix(y ~ x, data = pos_dat))
mm_pred <- matrix(nrow = 1, ncol = 2, data = c(1, 0))
obj <- MakeADFun(
  data = list(
    X1_ij = mm1, y1_i = d$non_zero,
    X2_ij = mm2, y2_i = log(pos_dat$y),
    X1_pred_ij = mm_pred, X2_pred_ij = mm_pred),
  parameters = list(b1_j = rep(0, ncol(mm1)), b2_j = rep(0, ncol(mm2)),
    log_sigma = log(0.2)),
  DLL = "deltalognormal")
obj$fn(obj$par)
obj$gr(obj$par)
st <- Sys.time()
opt <- nlminb(
  start = obj$par,
  objective = obj$fn,
  gradient = obj$gr)
r <- sdreport(obj)
ft <- Sys.time()
tmb_time1 <- as.numeric(ft - st, units = "secs")
```

That took `r round(tmb_time1, 3)` seconds.

```{r}
summary(r)
exp(summary(r)["log_prediction","Estimate"])
exp(summary(r)["log_prediction","Estimate"] +
    1.96 * summary(r)["log_prediction","Std. Error"])
exp(summary(r)["log_prediction","Estimate"] -
    1.96 * summary(r)["log_prediction","Std. Error"])
```

We get exactly the same point estimate as with the R functions.

We can compare the confidence intervals derived from the delta method standard errors from TMB (on the log scale) to a bootstrap approach:

```{r}
hurdle_fn <- function(data, i) {
  dat_boot <- data[i, ]
  m1 <- glm(non_zero ~ x, data = dat_boot,
    family = binomial(link = logit))
  m2 <- glm(y ~ x, data = subset(dat_boot, non_zero == 1),
    family = Gamma(link = log))
  bin_coef <- plogis(predict(m1, newdata = data.frame(x = 0)))
  gamma_coef <- exp(predict(m2, newdata = data.frame(x = 0)))
  bin_coef * gamma_coef
}

library(boot)
st <- Sys.time()
b <- boot(d, hurdle_fn, R = 1000)
# bca sometimes gives errors on big datasets but gives similar results on small datasets
b.ci <- boot.ci(b, type = "norm") 
ft <- Sys.time()
print(b.ci)
```

That took `r round(as.numeric(ft - st, units = "secs"), 3)` seconds 
(`r round(as.numeric(ft - st, units = "secs")/tmb_time1, 0)` times longer than the TMB approach.)

The confidence intervals are very similar. However, the bootstrap approach takes a very long time to fit to a full data set of real groundfish CPUE data. It's doable for one stock if you are very patient but likely prohibitive to run for all or many stocks. The TMB approach is much faster.

## A delta-lognormal TMB GLM fit to real CPUE data

Now let's try fitting our model to some real CPUE data. As an example, I will work with bottom trawl CPUE for walleye pollock in 5CDE so we have a point of comparison from the recent assessment. First I will use a GLM and LM (linear model) as above to relatively closely match the approach that was used in the assessment.

```{r}
grep_pattern <- "5[CDE]+"
if (!file.exists("d_retained.rda")) {
  source("R-draft/cpue-index.R")
  save(d_retained, file = "d_retained.rda") 
} else {
  load("d_retained.rda")
}
nrow(d_retained)
mean(d_retained$pos_catch)
```

Let's fit our models in TMB. First we will start with a simple model that just has year effects. 
```{r, message=FALSE, warning=FALSE, results="hide"}
compile("inst/tmb/deltalognormal.cpp")
dyn.load(dynlib("inst/tmb/deltalognormal"))

f1 <- pos_catch ~ year_factor
f2 <- log(spp_catch/hours_fished) ~ year_factor

if (!file.exists("cpue-glm-eg-year.rda")) {
  source("R-draft/delta-cpue-tmb-glm.R")
  save(r, mm1, mm2, obj, that_took, file = "cpue-glm-eg-year.rda") 
} else {
  load("cpue-glm-eg-year.rda")
}
```

```{r}
gg_tmb_cpue_ribbon <- function(report) {
  report_sum <- summary(report)
  ii <- grep("log_prediction", row.names(report_sum))
  row.names(report_sum) <- NULL
  df <- as.data.frame(report_sum[ii, ])
  df$year <- 1996:max(d_retained$year)
  
  df$Estimate <- df$Estimate - mean(df$Estimate)
  g <- ggplot(df, aes(year, exp(Estimate),
    ymin = exp(Estimate - 1.96 * `Std. Error`),
    ymax = exp(Estimate + 1.96 * `Std. Error`))) +
    geom_ribbon(alpha = 0.5) +
    geom_line()
  print(g)
  invisible(df)
}
df1 <- gg_tmb_cpue_ribbon(r)
```

Now we can fit a full model that matches the model used in the last stock assessment:

```{r, message=FALSE, warning=FALSE, results="hide"}
f <- function(x) as.factor(as.character(x))
f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
  f(dfo_locality) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(vessel_name) + f(depth_band) + f(latitude_band) +
  f(dfo_locality)

if (!file.exists("cpue-glm-eg.rda")) {
  source("R-draft/delta-cpue-tmb-glm.R")
  save(r, mm1, mm2, that_took, obj, file = "cpue-glm-eg.rda") 
} else {
  load("cpue-glm-eg.rda")
}
```

That took `r that_took` seconds.

```{r}
opt$convergence
max(r$gradient.fixed)
r2 <- r
df2 <- gg_tmb_cpue_ribbon(r2)
sm <- summary(r2)
```

Let's combine the predictions from the earlier simple model with this model to plot them. The model with just year effects is in blue:

```{r}
ggplot(df2, aes(year, exp(Estimate),
  ymin = exp(Estimate - 1.96 * `Std. Error`),
  ymax = exp(Estimate + 1.96 * `Std. Error`))) +
  geom_ribbon(alpha = 0.3) +
  geom_line() +
  geom_line(data = df1, col = "blue") +
  geom_ribbon(data = df1, fill = "blue", alpha = 0.3)
```

I also wrote a quick helper function to plot the coefficients from the models. The panels that start with "bin" are the binary models and the panel to start with "pos" are the positive component models. I'd have to do a bit more work to make sure the various binned factor predictors are always in order in the plot.

<!-- TODO -->

```{r, fig.height=12, fig.width=12}
source("R/gg_cpue_coefs.R")
gg_cpue_coefs(sm, mm1, mm2, re = FALSE)
```

## Evaluating the sensitivity to individual predictors

Let's try jackknifing out each of the predictor variables one at a time to see their influence on the final index trend. 

```{r jk, cache=TRUE}
f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
  f(dfo_locality) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(vessel_name) + f(depth_band) + f(latitude_band) +
  f(dfo_locality)

make_pred_mm <- function(x) {
  mm_pred <- x[1:length(unique(d_retained$year)), ]
  for (i in 1:ncol(mm_pred)) {
    for (j in 1:nrow(mm_pred)) {
      mm_pred[j, i] <- 0
    }}
  mm_pred[,1] <- 1
  for (i in 1:ncol(mm_pred)) {
    for (j in 1:nrow(mm_pred)) {
      if (i == j)
        mm_pred[j, i] <- 1
    }}
  mm_pred
}

jackknife_cpue <- function(f_bin, f_pos, terms) {
  
  mm1 <- model.matrix(f_bin, data = d_retained)
  mm2 <- model.matrix(f_pos, data = subset(d_retained, pos_catch == 1))
  mm1 <- make_pred_mm(mm1)
  mm2 <- make_pred_mm(mm2)
  
  m_bin <- speedglm::speedglm(f_bin, data = d_retained,
    family = binomial(link = "logit"))
  m_pos <- lm(f_pos, data = subset(d_retained, pos_catch == 1))
  
  p1 <- plogis(mm1 %*% coef(m_bin))
  p2 <- exp(mm2 %*% coef(m_pos))
  full <- data.frame(year = 1996:2015, term = "all", pred = p1 * p2)
  
  fitm <- function(drop_term) {
    f1 <- update.formula(formula(m_bin), as.formula(paste0(". ~ . -", drop_term)))
    f2 <- update.formula(formula(m_pos), as.formula(paste0(". ~ . -", drop_term)))
    mm1 <- model.matrix(f1, data = d_retained)
    mm2 <- model.matrix(f2, data = subset(d_retained, pos_catch == 1))
    mm1 <- make_pred_mm(mm1)
    mm2 <- make_pred_mm(mm2)
    m_bin <- speedglm::speedglm(f1, data = d_retained,
      family = binomial(link = "logit"))
    m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
    p1 <- plogis(mm1 %*% coef(m_bin))
    p2 <- exp(mm2 %*% coef(m_pos))
    o <- data.frame(year = 1996:2015, term = drop_term, pred = p1 * p2)
    o
  }
  
  out <- plyr::ldply(terms, fitm)
  suppressWarnings(out <- bind_rows(full, out))
  out <- group_by(out, term) %>% mutate(pred = pred / exp(mean(log(pred)))) %>% 
      ungroup()
  out
}

jk <- jackknife_cpue(f1, f2, terms = c("f(depth_band)", "f(vessel_name)", 
  "f(latitude_band)", "f(dfo_locality)", "f(month_factor)"))
```

```{r, dependson='jk'}
ggplot(jk, aes(year, pred, colour = term)) +
  geom_line(data = df1, aes(y = exp(Estimate - mean(Estimate))), colour = "black", lty = 2) +
  geom_line(aes(size = term)) +
  scale_color_manual(values = c("Black", RColorBrewer::brewer.pal(8, "Set2"))) +
  scale_size_manual(values = c(3, rep(0.75, 99)))
```

In the above plot, the dashed line is the unstandardized CPUE trend. The thick black line is the model with all of the predictors. Each of the colored lines represents a model with that individual term removed from the model.

Lines that deviate the furthest from the overall thick black line have the biggest influence on the model fit. From this plot we can see that depth has the biggest influence on standardizing the index series. Leaving out depth results in a index that isn't that far off from the unstandardized CPUE index series. DFO locality has a relatively big effect on the index in recent years. 

```{r eval=FALSE, include=FALSE}
# # coefs AIC:
# m_bin <- speedglm::speedglm(f1, data = d_retained, family = binomial(link = "logit"))
# m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
#
# m_bin2 <- update(m_bin, . ~ . + poly(best_depth, 4) - f(depth_band))
# bbmle::AICtab(m_bin, m_bin2)
#
# m_pos <- lm(f2, data = subset(d_retained, pos_catch == 1))
# m_pos2 <- update(m_pos, . ~ . + poly(best_depth, 8) - f(depth_band))
# bbmle::AICtab(m_pos, m_pos2)

# Let's compare that to a bootstrap approach:
# 
# f1 <- pos_catch ~ year_factor + f(month_factor) + f(vessel_name) +
#   f(dfo_locality) + f(depth_band) + f(latitude_band)
# f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
#   f(vessel_name) + f(depth_band) + f(latitude_band) +
#   f(dfo_locality)
# hurdle_fn <- function(data, i) {
#  dat_boot <- data[i, ]
#  # m_bin <- speedglm::speedglm(f1, data = d_retained,
#    # family = binomial(link = "logit"))
#  m_pos <- lm(f2, data = subset(dat_boot, pos_catch == 1))
#  pos_coef <- coef(m_pos)[1:20]
#  pos_coef
# }
# library(boot)
# b <- boot(d_retained, hurdle_fn, R = 100)
# b.ci <- boot.ci(b, type = "bca")
# b
# print(b.ci)

# Before calling mclapply do openmp(1) to avoid forking a multithreaded process
# TMB::openmp(1)
# parallel::mclapply(1:10,function(x)do.call("optim",obj))
# parallel_accumulator<Type> res(this);
# parallel_accumulator<Type> nll(this);
```

## A random effects model

Now let's fit a similar model with random intercepts for vessel and DFO locality instead of treating them factor-level predictors.

This is the model:

```{r, comment=NA,echo=FALSE}
writeLines(readLines("inst/tmb/deltalognormal_2re.cpp"))
```

```{r, message=FALSE, warning=FALSE, results="hide"}
compile("inst/tmb/deltalognormal_2re.cpp")
dyn.load(dynlib("inst/tmb/deltalognormal_2re"))

f1 <- pos_catch ~ year_factor + f(month_factor) + f(depth_band) + f(latitude_band)
f2 <- log(spp_catch/hours_fished) ~ year_factor + f(month_factor) +
  f(depth_band) + f(latitude_band)

if (!file.exists("cpue-glmm-eg.rda")) {
  source("R/delta-cpue-tmb-glmm.R")
  save(r_re, mm1, mm2, obj, that_took, file = "cpue-glmm-eg.rda") 
} else {
  load("cpue-glmm-eg.rda")
}
```

That took `r that_took` seconds.

That takes considerably longer to run. Let's make a plot combining the various trends we have calculated:

```{r}
df_re <- gg_tmb_cpue_ribbon(r_re)
sm_re <- summary(r_re)
ggplot(df2, aes(year, exp(Estimate),
  ymin = exp(Estimate - 1.96 * `Std. Error`),
  ymax = exp(Estimate + 1.96 * `Std. Error`))) +
  geom_ribbon(data = df_re, fill =  "red", alpha = 0.3) + 
  geom_ribbon(data = df1, fill = "blue", alpha = 0.3) +
  geom_ribbon(alpha = 0.3) +
  geom_line(data = df_re, col = "red") +
  geom_line(data = df1, col = "blue") +
  geom_line()
```

In the above plot, blue is the unstandardized series, black is the GLM/LM standardized series, and red is the GLMM/LMM standardized series (i.e. the red line and shading is for the model including random intercepts for vessels and DFO locality).

So the random effects model produces almost exactly the same point estimate of the index trend but introduces a bit of (likely realistic) uncertainty around the trend estimate. Importantly, the random effect model estimates its trend for an average locality and average vessel whereas the GLM version has to make its estimate at a particular locality and vessel. However, once they are all scaled to have the same geometric mean the results are quite similar. Given the extra time needed for the random effects model with a similar result, it may not be worth using.

Again, let's plot out the coefficients:

```{r, fig.height=12, fig.width=12}
gg_cpue_coefs(sm_re, mm1, mm2, re = TRUE)
```

# Comparing methods of combining positive and binary component models

I've been trying to work out the formula used to combine the positive and binary models in a number of recent groundfish stock assessments. 

The equation is:

lyy / (1 - p0 * (1 - 1/(byy)))

where lyy is the lognormal/positive component index (on the natural scale), p0 is the probability of observing the species in year 1, and byy is the binary component index (on the probability scale).

The following is a tiny simulation example:

```{r}
cyy <- function(p0, byy, lyy) {lyy / (1 - p0 * (1 - 1/(byy)))}
cyy_lower <- function(p0, byy) {(1 - p0 * (1 - 1/(byy)))}

set.seed(1)
p <- rnorm(20, mean = 0)
b <- rnorm(20, mean = 1)
plot(scale(cyy(plogis(p[1]), plogis(b), exp(p))), type = "l", lty = 2, col = "red")
lines(scale(plogis(b) * exp(p)), type = "l", lty = 1)
```

In the above plot, the red line is the version combined according to the formula and the black line is the version combined by multiplying the probability by the positive mean expectation. The results are similar (after scaling) but not exactly the same. 

We can also just plot out the lower half of the formula that scales and inverts the probability component to see what's going on.

```{r}
plot(1/(cyy_lower(plogis(p[1]), plogis(b))), type = "l", lty = 2, col = "red",
  ylim = c(0, 1))
lines(plogis(b))
```

In the above plot, the red line is the version from the formula and the black line is the raw probability. I still can't wrap my head around why we'd want to use the formula used in a number of recent assessments versus just multiplying the probability by the positive estimate.
